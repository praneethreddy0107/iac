{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "     \n",
    "Get started with BigQuery DataFrames\n",
    "Colab logo Run in Colab\tGitHub logo View on GitHub\tVertex AI logo Open in Vertex AI Workbench\n",
    "NOTE: This notebook has been tested in the following environment:\n",
    "\n",
    "Python version = 3.10\n",
    "Overview\n",
    "Use this notebook to get started with BigQuery DataFrames, including setup, installation, and basic tutorials.\n",
    "\n",
    "BigQuery DataFrames provides a Pythonic DataFrame and machine learning (ML) API powered by the BigQuery engine.\n",
    "\n",
    "bigframes.pandas provides a pandas-like API for analytics.\n",
    "bigframes.ml provides a scikit-learn-like API for ML.\n",
    "Learn more about BigQuery DataFrames.\n",
    "\n",
    "Objective\n",
    "In this tutorial, you learn how to install BigQuery DataFrames, load data into a BigQuery DataFrames DataFrame, and inspect and manipulate the data using pandas and a custom Python function, running at BigQuery scale.\n",
    "\n",
    "The steps include:\n",
    "\n",
    "Creating a BigQuery DataFrames DataFrame: Access data from a Parquet file stored in GCS to create a BigQuery DataFrames DataFrame.\n",
    "Inspecting and manipulating data: Use pandas to perform data cleaning and preparation on the DataFrame.\n",
    "Deploying a custom function: Deploy a remote function that runs a scalar Python function at BigQuery scale.\n",
    "Dataset\n",
    "This tutorial uses the Jump Start Solution dataset (thelook), which contains syntheic retail sales data for a fictional eCommerce clothing retailer.\n",
    "\n",
    "The same dataset is also deployed to a Cloud Storage bucket in your project as Parquet files so that you can use it to try ingesting data from a local environment.\n",
    "\n",
    "Costs\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "BigQuery (storage and compute)\n",
    "Cloud Functions\n",
    "Learn about BigQuery storage pricing, BigQuery compute pricing, and Cloud Functions pricing, and use the Pricing Calculator to generate a cost estimate based on your projected usage.\n",
    "\n",
    "Before you begin\n",
    "Complete the tasks in this section to set up your environment.\n",
    "\n",
    "Set your project ID\n",
    "If you don't know your project ID, try the following:\n",
    "\n",
    "Run gcloud config list.\n",
    "Run gcloud projects list.\n",
    "See the support page: Locate the project ID.\n",
    "\n",
    "PROJECT_ID = ${PROJECT_ID}\n",
    "     \n",
    "Set the region\n",
    "You can also change the REGION variable used by BigQuery. Learn more about BigQuery regions.\n",
    "\n",
    "\n",
    "REGION = ${REGION}\n",
    "     \n",
    "Import libraries\n",
    "\n",
    "import bigframes.pandas as bf\n",
    "     \n",
    "Set BigQuery DataFrames options\n",
    "\n",
    "bf.options.bigquery.project = PROJECT_ID\n",
    "bf.options.bigquery.location = REGION\n",
    "     \n",
    "If you want to reset the location of the created DataFrame or Series objects, reset the session by executing bf.close_session(). After that, you can reuse bf.options.bigquery.location to specify another location.\n",
    "\n",
    "See the power of BigQuery DataFrames first-hand\n",
    "BigQuery DataFrames enables you to interact with datasets of any size, so that you can explore, transform, and understand even your biggest datasets using familiar tools like pandas and scikit-learn.\n",
    "\n",
    "For example, take the BigQuery sample table bigquery-samples.wikipedia_pageviews.200809h, which is ~60 GB is size. This is not a dataset you'd likely be able process in pandas without extra infrastructure.\n",
    "\n",
    "With BigQuery DataFrames, however, computation is handled by BigQuery's highly scalable compute engine, meaning you can focus on doing data science without hitting size limitations.\n",
    "\n",
    "If you'd like to try creating a BigQuery DataFrames DataFrame from this table, uncomment and run the next cell to load the table using the read_gbq method.\n",
    "\n",
    "Note: Keep in mind that running these operations will count against your monthly free tier allowance in BigQuery.\n",
    "\n",
    "\n",
    "# bq_df_sample = bf.read_gbq(\"bigquery-samples.wikipedia_pageviews.200809h\")\n",
    "     \n",
    "No problem! BigQuery DataFrames makes a DataFrame, bq_df_sample, containing the entirety of the source table of data.\n",
    "\n",
    "Uncomment and run the following cell to see pandas in action over your new BigQuery DataFrames DataFrame.\n",
    "\n",
    "This code uses regex to filter the DataFrame to include only rows with Wikipedia page titles containing the word \"Google\", sums the total views by page title, and then returns the top 100 results.\n",
    "\n",
    "\n",
    "# bq_df_sample[bq_df_sample.title.str.contains(r\"[Gg]oogle\")]\\\n",
    "# .groupby(['title'], as_index=False)['views'].sum(numeric_only=True)\\\n",
    "# .sort_values('views', ascending=False)\\\n",
    "# .head(100)\n",
    "     \n",
    "In addition to giving you access to pandas, BigQuery DataFrames also enables you to build ML models, run inference, and deploy and run your own Python functions at scale. You'll see examples throughout this notebook.\n",
    "\n",
    "Now you'll move to the JSS dataset (thelook) for the remainder of this getting started guide.\n",
    "\n",
    "Create a BigQuery DataFrames DataFrame\n",
    "You can create a BigQuery DataFrames DataFrame by reading data from any of the following locations:\n",
    "\n",
    "A data file stored in Cloud Storage\n",
    "Data stored in a BigQuery table\n",
    "A local data file\n",
    "An in-memory pandas DataFrame\n",
    "The following sections show how to use the first two options.\n",
    "\n",
    "Create a DataFrame from a GCS file\n",
    "Use these instructions in the following sections to create a BigQuery DataFrames DataFrame from a file stored in Google Cloud Storage.\n",
    "\n",
    "Define the file path\n",
    "First, follow the steps below to define the Parquet file URI.\n",
    "\n",
    "\n",
    "fn_order_items = \"${GCS_BUCKET_URI}/thelook-ecommerce/order_items.parquet\"\n",
    "     \n",
    "Create a DataFrame\n",
    "Create a BigQuery DataFrames DataFrame from the parquet file:\n",
    "\n",
    "\n",
    "df_from_gcs = bf.read_parquet(path=fn_order_items)\n",
    "     \n",
    "Take a look at the first few rows of the orders DataFrame that was just created:\n",
    "\n",
    "\n",
    "df_from_gcs.head()\n",
    "     \n",
    "Ingest data from a DataFrame to a BigQuery table\n",
    "BigQuery DataFrames lets you create a BigQuery table from a BigQuery DataFrames DataFrame on-the-fly.\n",
    "\n",
    "First, create a BigQuery dataset to house the table.\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "DATASET_ID = \"thelook_bigframes\"\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "dataset = bigquery.Dataset(PROJECT_ID + \".\" + DATASET_ID)\n",
    "client.delete_dataset(DATASET_ID, delete_contents=True, not_found_ok=True)\n",
    "dataset.location = REGION\n",
    "dataset = client.create_dataset(dataset)\n",
    "print(f\"Dataset {dataset.dataset_id} created.\")\n",
    "     \n",
    "Dataset thelook_bigframes created.\n",
    "Next, use the to_gbq method to create a BigQuery table from the DataFrame:\n",
    "\n",
    "\n",
    "df_from_gcs.to_gbq(PROJECT_ID + \".\" + DATASET_ID + \".order_items\")\n",
    "     \n",
    "Create a DataFrame from BigQuery data\n",
    "You can create a BigQuery DataFrames DataFrame from a BigQuery table by using the read_gbq method and referencing either an entire table or a SQL query.\n",
    "\n",
    "Create a BigQuery DataFrames DataFrame from the BigQuery table you created in the previous section, and view a few rows:\n",
    "\n",
    "\n",
    "query_or_table = f\"{PROJECT_ID}.{DATASET_ID}.order_items\"\n",
    "bq_df = bf.read_gbq(query_or_table)\n",
    "bq_df[\"order_id\"] = bq_df[\"order_id\"].astype(\"string\")\n",
    "bq_df.head()\n",
    "     \n",
    "Inspect and manipulate data in BigQuery DataFrames\n",
    "Using pandas\n",
    "You can use pandas as you normally would on the BigQuery DataFrames DataFrame, but calculations happen in the BigQuery query engine instead of your local environment. There are 150+ pandas functions supported in BigQuery DataFrames. You can view the list in the documentation.\n",
    "\n",
    "To see this in action, inspect one of the columns (or series) of the BigQuery DataFrames DataFrame:\n",
    "\n",
    "\n",
    "bq_df[\"sale_price\"].head(10)\n",
    "     \n",
    "Compute the sum of this series to find the total reveneue from all items sold:\n",
    "\n",
    "\n",
    "total_sales = bq_df[\"sale_price\"].sum()\n",
    "print(f\"total_sales: {total_sales}\")\n",
    "     \n",
    "Calculate the mean sales_price of all items in an order using the groupby operation to group by order_id:\n",
    "\n",
    "\n",
    "bq_df[[\"order_id\", \"sale_price\"]].groupby(\n",
    "    by=bq_df[\"order_id\"]).mean(numeric_only=True).head()\n",
    "     \n",
    "You can confirm that the calculations were run in BigQuery by clicking \"Open job\" from the previous cells' output. This takes you to the BigQuery console to view the SQL statement and job details.\n",
    "\n",
    "Using custom functions\n",
    "Running your own Python functions (or being able to bring your packages) and using them at scale is a challenge many data scientists face. BigQuery DataFrames makes it easy to deploy remote functions that run scalar Python functions at BigQuery scale. These functions are persisted as BigQuery remote functions that you can then re-use.\n",
    "\n",
    "Running the cell below creates a custom function using the remote_function method. This function categorizes a value into one of two buckets: >= 50 or <50.\n",
    "\n",
    "Note: Creating a function requires a BigQuery connection. This code assumes a pre-created connection named bigframes-default-connection. If the connection is not already created, BigQuery DataFrames attempts to create one assuming the necessary APIs and IAM permissions are set up in the project.\n",
    "\n",
    "This cell takes a few minutes to run because it creates the BigQuery connection (if applicable) and deploys the Cloud Function.\n",
    "\n",
    "\n",
    "@bf.remote_function([float], str)\n",
    "def get_bucket(num):\n",
    "  if not num:\n",
    "    return \"NA\"\n",
    "  boundary = 50\n",
    "  return \"at_or_above_50\" if num >= boundary else \"below_50\"\n",
    "     \n",
    "The custom function is deployed as a Cloud Function, and is then integrated with BigQuery as a remote function.\n",
    "\n",
    "Save both of the function names so that you can clean them up at the end of this notebook.\n",
    "\n",
    "\n",
    "CLOUD_FUNCTION_NAME = format(get_bucket.bigframes_cloud_function)\n",
    "print(\"Cloud Function Name \" + CLOUD_FUNCTION_NAME)\n",
    "REMOTE_FUNCTION_NAME = format(get_bucket.bigframes_remote_function)\n",
    "print(\"Remote Function Name \" + REMOTE_FUNCTION_NAME)\n",
    "     \n",
    "Apply the custom function to the BigQuery DataFrames DataFrame to bucketize the sale_price value of each item sold:\n",
    "\n",
    "\n",
    "bq_df = bq_df.assign(order_price_bucket=bq_df[\"sale_price\"].groupby(\n",
    "    by=bq_df[\"order_id\"]).sum().apply(get_bucket))\n",
    "bq_df[[\"order_id\", \"order_price_bucket\"]].head(10)\n",
    "     \n",
    "Summary and next steps\n",
    "You've created BigQuery DataFrames DataFrames, and inspected and manipulated data with pandas and custom remote functions at BigQuery scale and speed.\n",
    "\n",
    "Learn more about BigQuery DataFrames in the documentation and find more sample notebooks in the GitHub repo, including an introductory notebook for bigframes.ml.\n",
    "\n",
    "Cleaning up\n",
    "To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project you used for the tutorial.\n",
    "\n",
    "Otherwise, you can uncomment the remaining cells and run them to delete the individual resources you created in this tutorial:\n",
    "\n",
    "\n",
    "# # Delete the BigQuery dataset\n",
    "# from google.cloud import bigquery\n",
    "# client = bigquery.Client(project=PROJECT_ID)\n",
    "# client.delete_dataset(\n",
    "#  DATASET_ID, delete_contents=True, not_found_ok=True\n",
    "# )\n",
    "# print(\"Deleted dataset '{}'.\".format(DATASET_ID))\n",
    "     \n",
    "\n",
    "# # Delete the BigQuery Connection\n",
    "# from google.cloud import bigquery_connection_v1 as bq_connection\n",
    "# client = bq_connection.ConnectionServiceClient()\n",
    "# CONNECTION_ID = f\"projects/{PROJECT_ID}/locations/{REGION}/connections/bigframes-default-connection\"\n",
    "# client.delete_connection(name=CONNECTION_ID)\n",
    "# print(\"Deleted connection '{}'.\".format(CONNECTION_ID))\n",
    "     \n",
    "\n",
    "# # Delete the Cloud Function\n",
    "# ! gcloud functions delete {CLOUD_FUNCTION_NAME} --quiet\n",
    "     \n",
    "\n",
    "# # Delete the Remote Function\n",
    "# REMOTE_FUNCTION_NAME = REMOTE_FUNCTION_NAME.replace(PROJECT_ID + \".\", \"\")\n",
    "# ! bq rm --routine --force=true {REMOTE_FUNCTION_NAME}\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
